{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      precip  solar   air  vapor     label\n",
      "0        0.0  359.6   6.2    5.0  0.000000\n",
      "1        0.0  334.0   6.0    4.8  0.000000\n",
      "2        0.0  347.7   6.2    5.0  0.000000\n",
      "3        0.0  335.8  14.0    7.7  0.000000\n",
      "4        0.4  319.7  15.6   10.0  0.000000\n",
      "...      ...    ...   ...    ...       ...\n",
      "2766     0.1   51.4 -16.2    1.4  0.142646\n",
      "2767     0.0   59.1 -17.2    1.3  0.140948\n",
      "2768     0.2   24.4 -13.7    1.9  0.139318\n",
      "2769     0.1   33.7  -3.1    3.8  0.138547\n",
      "2770     3.1    4.1   1.3    6.2  0.138313\n",
      "\n",
      "[2771 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "_dir = os.path.abspath('')\n",
    "data_path = os.path.join(_dir, \"../data/daily_cleaned.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "new_columns = df.columns.values\n",
    "new_columns[-1] = 'label'\n",
    "df.columns = new_columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series_to_supervised() function\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence: (t-n, ..., t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ..., t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # concatenate together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)   var5(t)\n",
      "1           0.0      359.6        6.2        5.0   0.000000  0.000000\n",
      "2           0.0      334.0        6.0        4.8   0.000000  0.000000\n",
      "3           0.0      347.7        6.2        5.0   0.000000  0.000000\n",
      "4           0.0      335.8       14.0        7.7   0.000000  0.000000\n",
      "5           0.4      319.7       15.6       10.0   0.000000  0.000000\n",
      "...         ...        ...        ...        ...        ...       ...\n",
      "2766        0.2       49.5      -17.6        1.3   0.144958  0.142646\n",
      "2767        0.1       51.4      -16.2        1.4   0.142646  0.140948\n",
      "2768        0.0       59.1      -17.2        1.3   0.140948  0.139318\n",
      "2769        0.2       24.4      -13.7        1.9   0.139318  0.138547\n",
      "2770        0.1       33.7       -3.1        3.8   0.138547  0.138313\n",
      "\n",
      "[2770 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# testing series to supervised\n",
    "values = df.values\n",
    "data = series_to_supervised(values)\n",
    "# drop the columns we don't want to predict (predicting for current time step), so all vars at time t except var5(t)\n",
    "data.drop(data.columns[[5, 6, 7, 8]], axis=1, inplace=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validation, test sets\n",
    "values = data.values\n",
    "train_df = values[:1386, :]\n",
    "valid_df = values[1386:2079, :]\n",
    "test_df = values[2079:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1386, 1, 5) (1386,) (693, 1, 5) (693,) (691, 1, 5) (691,)\n"
     ]
    }
   ],
   "source": [
    "# setup train data\n",
    "train_x, train_y = train_df[:, :-1], train_df[:, -1] # raw numpy\n",
    "\n",
    "# setup validation data\n",
    "valid_x, valid_y = valid_df[:, :-1], valid_df[:, -1]\n",
    "\n",
    "# setup test data\n",
    "test_x, test_y = test_df[:, :-1], test_df[:, -1]\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "# reshape inputs (x's) to be 3D [seq_len, batch, input_size]\n",
    "# using batches of 30, sequence length should always be 1\n",
    "# should be (30, 1, m)\n",
    "train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "valid_x = valid_x.reshape((valid_x.shape[0], 1, valid_x.shape[1]))\n",
    "test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
    "\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_2884]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-550b5772ebcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# plot the history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_2884]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# begin keras training...\n",
    "# create keras model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit the model\n",
    "history = model.fit(train_x, train_y, epochs=50, batch_size=30, validation_data=(valid_x, valid_y), verbose=2, shuffle=False)\n",
    "# plot the history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pytorch to use cuda (gpu training) if possible\n",
    "# pytorch stuff\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# convert to torch tensors\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "\n",
    "test_x = torch.from_numpy(test_x).float().to(device)\n",
    "test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape, test_x.shape, test_y.shape)\n",
    "\"\"\"\n",
    "# convert to torch datasets\n",
    "# train data\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "# validation data\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "# test data\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# setup DataLoaders\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup architecture of LSTM model\n",
    "class SoilNet(nn.Module):\n",
    "    def __init__(self, feature_size, output_size, hidden_size, seq_len, n_layers=2):\n",
    "        super(SoilNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_len\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(feature_size, hidden_size, n_layers) # LSTM layer\n",
    "        self.predict = nn.Linear(hidden_size, output_size) # output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(x), self.seq_len, -1), self.hidden)\n",
    "        last_time_step = lstm_out.view(self.seq_len, len(x), self.hidden_size)[-1]\n",
    "        y_pred = self.predict(last_time_step)\n",
    "        return y_pred\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        self.hidden = (torch.zeros(self.n_layers, self.seq_len, self.hidden_size).float().to(device), torch.zeros(self.n_layers, self.seq_len, self.hidden_size).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "SoilNet(\n",
      "  (lstm): LSTM(5, 50, num_layers=2)\n",
      "  (predict): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define arguments and instantiate model\n",
    "feature_size = 5 # 5 features\n",
    "output_size = 1 # just output a number\n",
    "hidden_dim = 50 # size of hidden state and cell state at each time step\n",
    "seq_len = 1\n",
    "n_layers = 2\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = SoilNet(feature_size, output_size, hidden_dim, seq_len, n_layers)\n",
    "model = model.float()\n",
    "model.to(device)\n",
    "\n",
    "# hyperparams\n",
    "lr=0.005\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1386])) that is different to the input size (torch.Size([1386, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# now start the training\n",
    "epochs = 10\n",
    "train_hist = np.zeros(epochs)\n",
    "\n",
    "model.train() # set to training mode\n",
    "for i in range(epochs):\n",
    "    model.init_hidden()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(train_x)\n",
    "    loss = criterion(y_pred.float(), train_y)\n",
    "    \n",
    "    train_hist[i] = loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  [0.005716411584264437, 0.004094231013504453, 0.004811074350466154, 0.002719409148891785]\n",
      "Mean test loss:  0.004335281524281708\n",
      "Predictions:\n",
      "[0.12422963102828122, 0.07597662783819323, 0.06288123502311305, 0.09041999476568373, 0.15030129989379576, 0.05322136544208819, 0.08266845679021903, 0.01890145984519781, 0.03230656745696325, 0.06316190664256519, 0.16552776916354603, 0.13374768384265248, 0.1483001103496152, 0.06649637069491482, 0.10380046530567676, 0.08897690126704091, 0.10861928266783023, 0.02876654099225038, 0.061777439367065964, 0.17629317428925878, 0.030901829904294804, 0.13954610532173262, 0.0732079012001161, 0.058419058777066595]\n",
      "\n",
      "Actual values:\n",
      "[0.130342251563586, 0.175548141086749, 0.0985124327956989, 0.179852501737318, 0.190332661290323, 0.169309059233449, 0.16356132392473097, 0.0461615750169722, 0.15219724462365603, 0.103205309139785, 0.132499652294854, 0.11923870532703999, 0.081446590118302, 0.0993261088709677, 0.148817126269956, 0.130387936827957, 0.17281412037037, 0.153092361111111, 0.0865833333333333, 0.15846438172042998, 0.0881411290322581, 0.12638180901143198, 0.0364564648729447, 0.161292534722222]\n"
     ]
    }
   ],
   "source": [
    "# now check accuracy for test set...\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    y_pred = model(inputs.cuda())\n",
    "    \n",
    "    test_loss = criterion(y_pred, labels.cuda())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred_cpu = y_pred.cpu()\n",
    "    pred_cpu = pred_cpu.detach().numpy()[:, 0]\n",
    "    pred_cpu = pred_cpu.tolist()\n",
    "    \n",
    "    lab_cpu = labels.cpu()\n",
    "    lab_cpu = lab_cpu.numpy()[:, 0]\n",
    "    lab_cpu = lab_cpu.tolist()\n",
    "    \n",
    "    predictions.extend(pred_cpu)\n",
    "    actuals.extend(lab_cpu)\n",
    "    \n",
    "\n",
    "print(\"Test loss: \", test_losses)\n",
    "print(\"Mean test loss: \", np.mean(test_losses))\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n",
    "print()\n",
    "print(\"Actual values:\")\n",
    "print(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
