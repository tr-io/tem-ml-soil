{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6288e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      precip  solar   air  vapor     label\n",
      "0        0.0  359.6   6.2    5.0  0.000000\n",
      "1        0.0  334.0   6.0    4.8  0.000000\n",
      "2        0.0  347.7   6.2    5.0  0.000000\n",
      "3        0.0  335.8  14.0    7.7  0.000000\n",
      "4        0.4  319.7  15.6   10.0  0.000000\n",
      "...      ...    ...   ...    ...       ...\n",
      "2766     0.1   51.4 -16.2    1.4  0.142646\n",
      "2767     0.0   59.1 -17.2    1.3  0.140948\n",
      "2768     0.2   24.4 -13.7    1.9  0.139318\n",
      "2769     0.1   33.7  -3.1    3.8  0.138547\n",
      "2770     3.1    4.1   1.3    6.2  0.138313\n",
      "\n",
      "[2771 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# load the data\n",
    "_dir = os.path.abspath('')\n",
    "data_path = os.path.join(_dir, \"../../data/daily_cleaned.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "new_columns = df.columns.values\n",
    "new_columns[-1] = 'label'\n",
    "df.columns = new_columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14854e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series_to_supervised() function\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence: (t-n, ..., t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ..., t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # concatenate together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ac58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)   var5(t)\n",
      "1           0.0      359.6        6.2        5.0   0.000000  0.000000\n",
      "2           0.0      334.0        6.0        4.8   0.000000  0.000000\n",
      "3           0.0      347.7        6.2        5.0   0.000000  0.000000\n",
      "4           0.0      335.8       14.0        7.7   0.000000  0.000000\n",
      "5           0.4      319.7       15.6       10.0   0.000000  0.000000\n",
      "...         ...        ...        ...        ...        ...       ...\n",
      "2766        0.2       49.5      -17.6        1.3   0.144958  0.142646\n",
      "2767        0.1       51.4      -16.2        1.4   0.142646  0.140948\n",
      "2768        0.0       59.1      -17.2        1.3   0.140948  0.139318\n",
      "2769        0.2       24.4      -13.7        1.9   0.139318  0.138547\n",
      "2770        0.1       33.7       -3.1        3.8   0.138547  0.138313\n",
      "\n",
      "[2770 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# testing series to supervised\n",
    "values = df.values\n",
    "data = series_to_supervised(values)\n",
    "# drop the columns we don't want to predict (predicting for current time step), so all vars at time t except var5(t)\n",
    "data.drop(data.columns[[5, 6, 7, 8]], axis=1, inplace=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662f9e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1386, 1, 5) (1386,) (693, 1, 5) (693,) (691, 1, 5) (691,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train, validation, test sets\n",
    "values = data.values\n",
    "train_df = values[:1386, :]\n",
    "valid_df = values[1386:2079, :]\n",
    "test_df = values[2079:, :]\n",
    "\n",
    "# setup train data\n",
    "train_x, train_y = train_df[:, :-1], train_df[:, -1] # raw numpy\n",
    "\n",
    "# setup validation data\n",
    "valid_x, valid_y = valid_df[:, :-1], valid_df[:, -1]\n",
    "\n",
    "# setup test data\n",
    "test_x, test_y = test_df[:, :-1], test_df[:, -1]\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "# reshape inputs (x's) to be 3D [seq_len, batch, input_size]\n",
    "# using batches of 30, sequence length should always be 1\n",
    "# should be (30, 1, m)\n",
    "train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "valid_x = valid_x.reshape((valid_x.shape[0], 1, valid_x.shape[1]))\n",
    "test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
    "\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb7cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1386, 1, 5]) torch.Size([1386]) torch.Size([693, 1, 5]) torch.Size([693])\n"
     ]
    }
   ],
   "source": [
    "# setup pytorch to use cuda (gpu training) if possible\n",
    "# pytorch stuff\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# convert to torch tensors\n",
    "train_x = torch.from_numpy(train_x).float().to(device)\n",
    "train_y = torch.from_numpy(train_y).float().to(device)\n",
    "\n",
    "valid_x = torch.from_numpy(valid_x).float().to(device)\n",
    "valid_y = torch.from_numpy(valid_y).float().to(device)\n",
    "\n",
    "#test_x = torch.from_numpy(test_x).float().to(device)\n",
    "#test_y = torch.from_numpy(test_y).float().to(device)\n",
    "\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59fd05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d7b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3357c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup architecture of LSTM model\n",
    "class SoilNet(nn.Module):\n",
    "    def __init__(self, feature_size, output_size, hidden_size, seq_len, n_layers=2):\n",
    "        super(SoilNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_len\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(feature_size, hidden_size, n_layers) # LSTM layer\n",
    "        self.predict = nn.Linear(hidden_size, output_size) # output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(x), self.seq_len, -1), self.hidden)\n",
    "        last_time_step = lstm_out.view(self.seq_len, len(x), self.hidden_size)[-1]\n",
    "        y_pred = self.predict(last_time_step)\n",
    "        return y_pred\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        self.hidden = (torch.zeros(self.n_layers, self.seq_len, self.hidden_size).float().to(device), torch.zeros(self.n_layers, self.seq_len, self.hidden_size).float().to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1bf3870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "SoilNet(\n",
      "  (lstm): LSTM(5, 50, num_layers=2)\n",
      "  (predict): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define arguments and instantiate model\n",
    "feature_size = 5 # 5 features\n",
    "output_size = 1 # just output a number\n",
    "hidden_dim = 50 # size of hidden state and cell state at each time step\n",
    "seq_len = 1\n",
    "n_layers = 2\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = SoilNet(feature_size, output_size, hidden_dim, seq_len, n_layers)\n",
    "model = model.float()\n",
    "model.to(device)\n",
    "\n",
    "# hyperparams\n",
    "lr=0.005\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1b67fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1386])) that is different to the input size (torch.Size([1386, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# now start the training\n",
    "epochs = 10\n",
    "train_hist = np.zeros(epochs)\n",
    "\n",
    "model.train() # set to training mode\n",
    "for i in range(epochs):\n",
    "    model.init_hidden()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(train_x)\n",
    "    loss = criterion(y_pred.float(), train_y)\n",
    "    \n",
    "    train_hist[i] = loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa8faa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  [13.65597945560748, 7.734004613786757, 0.7273761735863438, 0.7734169431540802, 2.6011529733412395, 2.0739807840347293, 0.5756662605945136, 0.49240585119494484, 0.07393020195841662, 0.04623877645675084, 0.660469800803461, 2.7693723330287874, 4.361440156601988, 3.6457905159513246, 0.7998593620198211, 0.801133891474986, 5.318818693074666, 0.8754843194349268, 2.09525239923398, 5.053307237885247, 6.138427271033548, 0.14013753458483247, 0.6744587165426525, 8.844123917686388e-05]\n",
      "Mean test loss:  2.587008029442694\n",
      "Predictions:\n",
      "[0.1558946967124939, 0.15910181403160095, 0.15660345554351807, 0.1477985978126526, 0.15446509420871735, 0.15802830457687378, 0.15843597054481506, 0.15827453136444092, 0.15863037109375, 0.15861007571220398, 0.1566115915775299, 0.15446969866752625, 0.15385359525680542, 0.1559169590473175, 0.1570504903793335, 0.1569717824459076, 0.15861272811889648, 0.158479243516922, 0.1561124324798584, 0.158480703830719, 0.15748170018196106, 0.16177883744239807, 0.16452404856681824, 0.16594451665878296, 0.16658669710159302, 0.16596630215644836, 0.16725656390190125, 0.15861201286315918, 0.16147994995117188, 0.1634405553340912, 0.16237449645996094, 0.1637430191040039, 0.1636585295200348, 0.16309234499931335, 0.16127991676330566, 0.1616910994052887, 0.16218659281730652, 0.1582161784172058, 0.1556839495897293, 0.15874242782592773, 0.15939092636108398, 0.15736660361289978, 0.15938326716423035, 0.15991666913032532, 0.1585211157798767, 0.15596607327461243, 0.15200568735599518, 0.15113265812397003, 0.15336337685585022, 0.1534130871295929, 0.1587287187576294, 0.15443025529384613, 0.15331411361694336, 0.1526167392730713, 0.15245063602924347, 0.14863961935043335, 0.1434430032968521, 0.14746931195259094, 0.15570935606956482, 0.15306809544563293, 0.15325331687927246, 0.153810054063797, 0.15442201495170593, 0.15358370542526245, 0.15221256017684937, 0.15139643847942352, 0.1505109667778015, 0.14970026910305023, 0.1477665901184082, 0.14877578616142273, 0.15003147721290588, 0.1520252823829651, 0.1538926064968109, 0.15239518880844116, 0.15639153122901917, 0.15821516513824463, 0.1587005853652954, 0.15921562910079956, 0.15766161680221558, 0.15825143456459045, 0.15865281224250793, 0.15820613503456116, 0.159147709608078, 0.1576155424118042, 0.15796950459480286, 0.15845412015914917, 0.15827816724777222, 0.15791934728622437, 0.15761783719062805, 0.15638050436973572, 0.15736600756645203, 0.15752831101417542, 0.1576426923274994, 0.15747606754302979, 0.15736517310142517, 0.15735629200935364, 0.1574951410293579, 0.15683770179748535, 0.15778738260269165, 0.15751034021377563, 0.15758374333381653, 0.15793263912200928, 0.15662506222724915, 0.15742281079292297, 0.15708810091018677, 0.15717238187789917, 0.15765568614006042, 0.15748432278633118, 0.1568448543548584, 0.15695413947105408, 0.1628594994544983, 0.1569579839706421, 0.15762671828269958, 0.15716668963432312, 0.15769004821777344, 0.1578904688358307, 0.15763506293296814, 0.1567484438419342, 0.15689843893051147, 0.1569846272468567, 0.1569576859474182, 0.15683528780937195, 0.15593701601028442, 0.1557656228542328, 0.15641626715660095, 0.1564323902130127, 0.15624666213989258, 0.15577369928359985, 0.15620654821395874, 0.15594574809074402, 0.1560010313987732, 0.1558222770690918, 0.15604239702224731, 0.1558913141489029, 0.1560610830783844, 0.15610310435295105, 0.15594589710235596, 0.15574371814727783, 0.15561139583587646, 0.15556025505065918, 0.15553522109985352, 0.1496511697769165, 0.14612412452697754, 0.15000149607658386, 0.1522369086742401, 0.14987008273601532, 0.1583329141139984, 0.15053042769432068, 0.1533088982105255, 0.15428635478019714, 0.15530569851398468, 0.15531718730926514, 0.15516646206378937, 0.15274080634117126, 0.15325242280960083, 0.15395744144916534, 0.15276232361793518, 0.15412315726280212, 0.1532239019870758, 0.15361642837524414, 0.15333032608032227, 0.15300168097019196, 0.1537102907896042, 0.1539078652858734, 0.15380185842514038, 0.15431657433509827, 0.15366604924201965, 0.1538630723953247, 0.15348224341869354, 0.1523098349571228, 0.15260830521583557, 0.15301361680030823, 0.153669536113739, 0.1533978283405304, 0.1538068652153015, 0.15308594703674316, 0.1536237895488739, 0.14971289038658142, 0.15193015336990356, 0.1533220410346985, 0.15432396531105042, 0.1543835997581482, 0.15375396609306335, 0.1482662558555603, 0.1514607071876526, 0.15339942276477814, 0.1540789008140564, 0.15346667170524597, 0.15311992168426514, 0.14464935660362244, 0.148805171251297, 0.14337794482707977, 0.14908891916275024, 0.15081587433815002, 0.1507074534893036, 0.14937973022460938, 0.1523633599281311, 0.15342140197753906, 0.15198108553886414, 0.15325430035591125, 0.15403500199317932, 0.154353529214859, 0.1540728062391281, 0.1611093282699585, 0.15459564328193665, 0.15373875200748444, 0.15423375368118286, 0.15297770500183105, 0.15418142080307007, 0.1544579565525055, 0.1543860137462616, 0.1482906937599182, 0.15233096480369568, 0.15370237827301025, 0.15380245447158813, 0.1530781090259552, 0.15027353167533875, 0.15251505374908447, 0.15221796929836273, 0.15313100814819336, 0.15384191274642944, 0.1538149118423462, 0.15263798832893372, 0.15299445390701294, 0.1514166295528412, 0.15267899632453918, 0.14523494243621826, 0.14605796337127686, 0.1487240493297577, 0.1521366983652115, 0.15334665775299072, 0.1524975597858429, 0.1493573784828186, 0.1512320339679718, 0.14990051090717316, 0.15074527263641357, 0.149818554520607, 0.14630445837974548, 0.147350013256073, 0.15010306239128113, 0.15099257230758667, 0.15004847943782806, 0.14929160475730896, 0.15121552348136902, 0.15451505780220032, 0.15566881000995636, 0.15573909878730774, 0.15585294365882874, 0.15618619322776794, 0.1563778519630432, 0.1564498245716095, 0.1563897430896759, 0.15651237964630127, 0.15419641137123108, 0.1511542797088623, 0.15408623218536377, 0.15458467602729797, 0.15609484910964966, 0.15072785317897797, 0.15258479118347168, 0.15674299001693726, 0.15068426728248596, 0.153381809592247, 0.15516504645347595, 0.1550053358078003, 0.1528584361076355, 0.15377312898635864, 0.15643349289894104, 0.15893083810806274, 0.1581498682498932, 0.15883511304855347, 0.159990131855011, 0.15530812740325928, 0.15729323029518127, 0.15600818395614624, 0.15122771263122559, 0.15511906147003174, 0.15764346718788147, 0.15897199511528015, 0.15464988350868225, 0.1516343057155609, 0.15289101004600525, 0.15543001890182495, 0.15674221515655518, 0.15698376297950745, 0.14763599634170532, 0.15053115785121918, 0.1538848876953125, 0.1566605269908905, 0.15594801306724548, 0.15286409854888916, 0.15250983834266663, 0.1481412649154663, 0.1518440544605255, 0.15524017810821533, 0.15629127621650696, 0.1549028754234314, 0.15424588322639465, 0.14529967308044434, 0.14882245659828186, 0.15557098388671875, 0.15737426280975342, 0.15733090043067932, 0.1595352590084076, 0.15155920386314392, 0.1433558166027069, 0.13167285919189453, 0.1455124169588089, 0.14540600776672363, 0.15081080794334412, 0.13259020447731018, 0.13347771763801575, 0.13505208492279053, 0.13610467314720154, 0.13899600505828857, 0.1393623948097229, 0.13565778732299805, 0.1325780749320984, 0.13788941502571106, 0.1444236934185028, 0.15326431393623352, 0.15238481760025024, 0.1452077329158783, 0.14306139945983887, 0.14589369297027588, 0.1484045833349228, 0.14387273788452148, 0.13739433884620667, 0.13623705506324768, 0.140508234500885, 0.1419365108013153, 0.1487712264060974, 0.14434361457824707, 0.14384880661964417, 0.1500522792339325, 0.15389856696128845, 0.14850017428398132, 0.14938578009605408, 0.15270015597343445, 0.15451322495937347, 0.15323400497436523, 0.15390250086784363, 0.15605908632278442, 0.15011093020439148, 0.14755156636238098, 0.14558766782283783, 0.14899566769599915, 0.15340152382850647, 0.15699854493141174, 0.1594035029411316, 0.15793395042419434, 0.14749909937381744, 0.14834024012088776, 0.15644055604934692, 0.15948674082756042, 0.15587759017944336, 0.15509366989135742, 0.1567424237728119, 0.156900554895401, 0.1580422818660736, 0.15817970037460327, 0.1577761471271515, 0.1585506796836853, 0.15870791673660278, 0.15809276700019836, 0.1541726142168045, 0.15587535500526428, 0.15763956308364868, 0.1581903100013733, 0.15715008974075317, 0.15708842873573303, 0.15257936716079712, 0.15198349952697754, 0.14918726682662964, 0.14832191169261932, 0.14891317486763, 0.14897793531417847, 0.15144428610801697, 0.15195077657699585, 0.15268036723136902, 0.1491081416606903, 0.149588942527771, 0.14919635653495789, 0.1504332423210144, 0.1517416536808014, 0.14924634993076324, 0.15144100785255432, 0.1534956395626068, 0.15421268343925476, 0.1539580523967743, 0.1541774868965149, 0.15308739244937897, 0.15101677179336548, 0.15147867798805237, 0.15220868587493896, 0.15280625224113464, 0.1525159776210785, 0.1520279049873352, 0.1510426104068756, 0.15054115653038025, 0.15033766627311707, 0.15068164467811584, 0.15195772051811218, 0.15478873252868652, 0.15424934029579163, 0.1563510298728943, 0.15745669603347778, 0.15826371312141418, 0.15860652923583984, 0.15820738673210144, 0.1585322618484497, 0.159641295671463, 0.15871772170066833, 0.15614137053489685, 0.1580662727355957, 0.15922150015830994, 0.15847763419151306, 0.15760797262191772, 0.15677356719970703, 0.1561613380908966, 0.15589886903762817, 0.15375861525535583, 0.15513211488723755, 0.15562944114208221, 0.15594854950904846, 0.1565212905406952, 0.15636855363845825, 0.15636992454528809, 0.1565101444721222, 0.15647247433662415, 0.15622952580451965, 0.1561354100704193, 0.1625763475894928, 0.1567126214504242, 0.15686485171318054, 0.15731766819953918, 0.15747645497322083, 0.15726631879806519, 0.15695405006408691, 0.15654200315475464, 0.15569408237934113, 0.15587711334228516, 0.15616044402122498, 0.1559271216392517, 0.15596899390220642, 0.15583904087543488, 0.15565240383148193, 0.1548299491405487, 0.15435203909873962, 0.14717595279216766, 0.15147414803504944, 0.15354657173156738, 0.15403440594673157, 0.1552688628435135, 0.1554381251335144, 0.15568454563617706, 0.1557668149471283, 0.15570546686649323, 0.1555391252040863, 0.1554153859615326, 0.15489590167999268, 0.1551445722579956, 0.155164897441864, 0.15497638285160065, 0.15500694513320923, 0.1548381745815277, 0.15501809120178223, 0.15505175292491913, 0.15473830699920654, 0.15461820363998413, 0.15451040863990784, 0.1540413349866867, 0.15447157621383667, 0.15460355579853058, 0.1545196771621704, 0.15475523471832275, 0.15487372875213623, 0.1538047194480896, 0.15439088642597198, 0.1544155776500702, 0.15480291843414307, 0.1547960340976715, 0.1548359990119934, 0.15484538674354553, 0.15462231636047363, 0.15478399395942688, 0.154808908700943, 0.15474477410316467, 0.15454494953155518, 0.15479472279548645, 0.15480303764343262, 0.1548365354537964, 0.15486201643943787, 0.1545960009098053, 0.1547284573316574, 0.1545318067073822, 0.1523718237876892, 0.15435650944709778, 0.15452492237091064, 0.15443772077560425, 0.1541384756565094, 0.1546393632888794, 0.15465468168258667, 0.1547519862651825, 0.15473806858062744, 0.15304619073867798, 0.14875468611717224, 0.15186285972595215, 0.15364429354667664, 0.15476050972938538, 0.15418390929698944, 0.14900878071784973, 0.14201435446739197, 0.14795547723770142, 0.15108555555343628, 0.15225175023078918, 0.15242737531661987, 0.15355375409126282, 0.15416795015335083, 0.15365588665008545, 0.1538173258304596, 0.15429672598838806, 0.15294061601161957, 0.15344646573066711, 0.1532202661037445, 0.15314385294914246, 0.15399911999702454, 0.15429583191871643, 0.1538924276828766, 0.15446779131889343, 0.1543218195438385, 0.15143024921417236, 0.1532222330570221, 0.15414491295814514, 0.15422329306602478, 0.14789962768554688, 0.1469331681728363, 0.15205985307693481, 0.15352249145507812, 0.1538364589214325, 0.15199005603790283, 0.1528020203113556, 0.14786472916603088, 0.15143828094005585, 0.15328577160835266, 0.15362131595611572, 0.1524544656276703, 0.15368670225143433, 0.15421226620674133, 0.15444760024547577, 0.15453413128852844, 0.15453121066093445, 0.15229828655719757, 0.15295520424842834, 0.15139508247375488, 0.1487448811531067, 0.15233378112316132, 0.15782484412193298, 0.15441371500492096, 0.15444061160087585, 0.15476593375205994, 0.1548522710800171, 0.1547301709651947, 0.15355107188224792, 0.15154093503952026, 0.15272556245326996, 0.151809960603714, 0.15282079577445984, 0.1534775197505951, 0.15246134996414185, 0.15175817906856537, 0.15320029854774475, 0.14848759770393372, 0.15105074644088745, 0.14747078716754913, 0.14730316400527954, 0.15111833810806274, 0.14749109745025635, 0.14088520407676697, 0.14892856776714325, 0.15272484719753265, 0.1528005748987198, 0.15336191654205322, 0.15193872153759003, 0.14867448806762695, 0.15194708108901978, 0.1538151204586029, 0.15385925769805908, 0.15399470925331116, 0.15426069498062134, 0.15446656942367554, 0.1471664309501648, 0.15315385162830353, 0.15447448194026947, 0.1607571244239807, 0.15604707598686218, 0.15704402327537537, 0.15616485476493835, 0.15522852540016174, 0.14903488755226135, 0.1521165817975998, 0.15375152230262756, 0.15630966424942017, 0.15750637650489807, 0.15715685486793518, 0.1567583978176117, 0.15571776032447815, 0.15610894560813904, 0.15669655799865723, 0.15621748566627502, 0.15269193053245544, 0.14906197786331177, 0.15246322751045227, 0.150619238615036, 0.15223583579063416, 0.15451937913894653, 0.15313288569450378, 0.15545442700386047, 0.15681356191635132, 0.15937164425849915, 0.15421301126480103, 0.1534404456615448, 0.15731558203697205, 0.1591307818889618, 0.1570209562778473, 0.15143747627735138, 0.1548258662223816, 0.15448614954948425, 0.15324994921684265, 0.1570391058921814, 0.15394577383995056, 0.15505915880203247, 0.1580386757850647, 0.15880444645881653, 0.15270337462425232, 0.15341505408287048, 0.1564876139163971, 0.15964075922966003, 0.16094756126403809, 0.16096749901771545, 0.16117209196090698, 0.16055050492286682, 0.15966251492500305, 0.15977981686592102, 0.15589138865470886, 0.15349185466766357, 0.15495985746383667, 0.14736559987068176, 0.15591180324554443, 0.1647144854068756, 0.1650632917881012, 0.16623547673225403, 0.1545342206954956, 0.155288964509964, 0.16057109832763672, 0.16412976384162903, 0.16393426060676575, 0.1642065942287445, 0.16096904873847961, 0.16301530599594116, 0.161300927400589, 0.1500985324382782, 0.1510850191116333, 0.15752968192100525, 0.14707663655281067, 0.1353914439678192, 0.1472265124320984, 0.15068204700946808, 0.14808166027069092, 0.15496858954429626, 0.14875644445419312, 0.15578830242156982, 0.14453229308128357, 0.13723766803741455, 0.13714328408241272, 0.1203979030251503, 0.12382260710000992, 0.12560473382472992, 0.1313260793685913, 0.14167240262031555, 0.14683318138122559, 0.1408006250858307, 0.145309716463089, 0.1509111225605011, 0.15300801396369934, 0.15279343724250793, 0.15331754088401794, 0.15312770009040833, 0.153991237282753, 0.14851725101470947, 0.14771682024002075]\n",
      "\n",
      "Actual values:\n",
      "[0.034828125, 0.0347239583333333, 0.03471875, 0.0351614583333333, 0.0355677083333333, 0.0362083333333333, 0.0364479166666667, 0.0362395833333333, 0.0360364583333333, 0.03596875, 0.034875, 0.0358802083333333, 0.0364270833333333, 0.036671875, 0.037000000000000005, 0.0371041666666667, 0.035453125, 0.0369583333333333, 0.03446875, 0.0366614583333333, 0.0365572916666667, 0.0364270833333333, 0.0362239583333333, 0.035859375, 0.0353645833333333, 0.03484375, 0.03484375, 0.0348072916666667, 0.035, 0.0349791666666667, 0.026609375, 0.0363854166666667, 0.039453125, 0.0395, 0.0395, 0.0393072916666667, 0.0388125, 0.040984375, 0.0523541666666667, 0.0547552083333333, 0.055078125, 0.0556979166666667, 0.0559270833333333, 0.0558229166666667, 0.0556041666666667, 0.05515625, 0.055234375, 0.0589166666666667, 0.0602708333333333, 0.0682135416666667, 0.07809375, 0.077078125, 0.077640625, 0.0850520833333333, 0.11940625, 0.151411458333333, 0.155916666666667, 0.150239583333333, 0.157536458333333, 0.153088541666667, 0.148822916666667, 0.145692708333333, 0.143125, 0.1395, 0.136473958333333, 0.140208333333333, 0.142239583333333, 0.153520833333333, 0.161036458333333, 0.158380208333333, 0.159333333333333, 0.164890625, 0.169734375, 0.17984375, 0.18527083333333302, 0.19561458333333304, 0.187484375, 0.18495833333333303, 0.20144270833333303, 0.18371354166666698, 0.18411458333333303, 0.202729166666667, 0.187984375, 0.18936979166666698, 0.19243229166666698, 0.194375, 0.19608333333333303, 0.194875, 0.19135416666666696, 0.190953125, 0.18877604166666698, 0.18624479166666696, 0.185197916666667, 0.18773958333333302, 0.19339583333333302, 0.201572916666667, 0.191234375, 0.19527083333333303, 0.190703125, 0.19634375, 0.19544270833333302, 0.1939375, 0.20109375, 0.190171875, 0.184671875, 0.18040104166666698, 0.17623958333333303, 0.17300000000000001, 0.17071875, 0.195, 0.193875, 0.189125, 0.187609375, 0.18109895833333303, 0.1751875, 0.17094791666666698, 0.17108333333333303, 0.16988020833333303, 0.166130208333333, 0.16234375, 0.158223958333333, 0.154479166666667, 0.152192708333333, 0.151890625, 0.148359375, 0.14400000000000002, 0.140697916666667, 0.137598958333333, 0.133177083333333, 0.128005208333333, 0.12271875, 0.117229166666667, 0.111614583333333, 0.106479166666667, 0.10101041666666699, 0.0952916666666667, 0.0897135416666667, 0.0853072916666667, 0.0816145833333333, 0.0744947916666667, 0.0749322916666667, 0.07615625, 0.0742395833333333, 0.0743489583333333, 0.0782083333333333, 0.080125, 0.0858385416666667, 0.0964635416666667, 0.10194270833333301, 0.0999739583333333, 0.096796875, 0.0931614583333333, 0.0903333333333333, 0.08984375, 0.08975, 0.09184375, 0.0907604166666667, 0.088953125, 0.104255208333333, 0.11599999999999999, 0.12396875, 0.122625, 0.117270833333333, 0.11215625, 0.10707291666666699, 0.102171875, 0.098578125, 0.0954583333333333, 0.10186458333333301, 0.11990625, 0.116453125, 0.11155208333333301, 0.107192708333333, 0.103786458333333, 0.10226041666666699, 0.11240104166666699, 0.113885416666667, 0.132692708333333, 0.138354166666667, 0.131744791666667, 0.125802083333333, 0.124234375, 0.142234375, 0.135270833333333, 0.128270833333333, 0.12211979166666699, 0.11666145833333301, 0.112661458333333, 0.110286458333333, 0.108411458333333, 0.105947916666667, 0.13697916666666698, 0.132817708333333, 0.140020833333333, 0.14740104166666698, 0.141296875, 0.135796875, 0.130885416666667, 0.12546875, 0.125359375, 0.122567708333333, 0.119473958333333, 0.140994791666667, 0.149114583333333, 0.146807291666667, 0.144536458333333, 0.139197916666667, 0.13525, 0.132067708333333, 0.128932291666667, 0.123385416666667, 0.124817708333333, 0.122734375, 0.120447916666667, 0.117307291666667, 0.114484375, 0.12284895833333301, 0.12405208333333301, 0.123671875, 0.122239583333333, 0.120229166666667, 0.11921875, 0.119786458333333, 0.11844270833333301, 0.116484375, 0.11821875, 0.138453125, 0.151713541666667, 0.150588541666667, 0.147255208333333, 0.143677083333333, 0.140916666666667, 0.139895833333333, 0.139890625, 0.139989583333333, 0.139890625, 0.145442708333333, 0.154182291666667, 0.158270833333333, 0.16407291666666698, 0.15715625, 0.154598958333333, 0.1524375, 0.149744791666667, 0.14759375, 0.147682291666667, 0.151817708333333, 0.153822916666667, 0.152348958333333, 0.150520833333333, 0.148640625, 0.14690625, 0.145338541666667, 0.146296875, 0.149552083333333, 0.149213541666667, 0.148276041666667, 0.14719791666666698, 0.156145833333333, 0.16078125, 0.157333333333333, 0.154984375, 0.152890625, 0.15426041666666698, 0.18072395833333302, 0.174177083333333, 0.167125, 0.16695833333333301, 0.1686875, 0.163614583333333, 0.160244791666667, 0.15748958333333302, 0.155447916666667, 0.154953125, 0.16505208333333302, 0.166614583333333, 0.162947916666667, 0.160463541666667, 0.158395833333333, 0.160255208333333, 0.16189583333333302, 0.160057291666667, 0.157734375, 0.155744791666667, 0.154098958333333, 0.16094270833333302, 0.160885416666667, 0.158864583333333, 0.157057291666667, 0.155447916666667, 0.162921875, 0.166625, 0.163958333333333, 0.161291666666667, 0.158822916666667, 0.156651041666667, 0.154630208333333, 0.15284375, 0.151364583333333, 0.149770833333333, 0.148380208333333, 0.14699479166666699, 0.1455, 0.143901041666667, 0.14259375, 0.141369791666667, 0.139484375, 0.138046875, 0.13670833333333302, 0.135088541666667, 0.133260416666667, 0.13061979166666698, 0.127515625, 0.123130208333333, 0.11946354166666699, 0.11773958333333301, 0.115848958333333, 0.114239583333333, 0.11325520833333301, 0.11230729166666699, 0.111145833333333, 0.11021875, 0.109578125, 0.10849479166666699, 0.106635416666667, 0.10524479166666699, 0.10436458333333301, 0.103458333333333, 0.102432291666667, 0.10153125, 0.10096354166666699, 0.100401041666667, 0.0999479166666667, 0.0989791666666667, 0.0976197916666667, 0.096171875, 0.0951302083333333, 0.094078125, 0.09259375, 0.0909479166666667, 0.08975, 0.088515625, 0.087046875, 0.0855677083333333, 0.084390625, 0.0831979166666667, 0.082015625, 0.0808020833333333, 0.0792083333333333, 0.0775364583333333, 0.0861510416666667, 0.135192708333333, 0.1318125, 0.1275, 0.12204166666666699, 0.11828645833333301, 0.11625, 0.114578125, 0.112421875, 0.110015625, 0.107255208333333, 0.103661458333333, 0.0990208333333333, 0.094140625, 0.0897708333333333, 0.0860572916666667, 0.0830416666666667, 0.0803645833333333, 0.077546875, 0.0746302083333333, 0.0718020833333333, 0.06965625, 0.068609375, 0.069484375, 0.0778802083333333, 0.0796458333333333, 0.0807291666666667, 0.0815052083333333, 0.0825208333333333, 0.0830833333333333, 0.083984375, 0.084546875, 0.0853072916666667, 0.0859739583333333, 0.0870989583333333, 0.0882239583333333, 0.0891041666666667, 0.0889635416666667, 0.0890885416666667, 0.0891145833333333, 0.0885052083333333, 0.0873541666666667, 0.08590625, 0.0842760416666667, 0.0826979166666667, 0.0811354166666667, 0.0795833333333333, 0.077828125, 0.0765, 0.0762135416666667, 0.0765833333333333, 0.0780625, 0.088390625, 0.0938854166666667, 0.095234375, 0.09534375, 0.0956822916666667, 0.0958854166666667, 0.096078125, 0.0962135416666667, 0.0955260416666667, 0.0945729166666667, 0.093296875, 0.091578125, 0.0893645833333333, 0.0921822916666667, 0.112453125, 0.147348958333333, 0.173921875, 0.18372395833333302, 0.180984375, 0.18330729166666698, 0.18605208333333303, 0.17995833333333303, 0.18443229166666697, 0.183359375, 0.17547916666666696, 0.17288541666666699, 0.17709895833333303, 0.18402083333333302, 0.18461458333333303, 0.18194270833333304, 0.18576041666666698, 0.18961458333333303, 0.19016145833333303, 0.18936979166666698, 0.185734375, 0.17985416666666698, 0.18430208333333303, 0.18525, 0.18726041666666698, 0.187578125, 0.18470833333333303, 0.18389583333333304, 0.18265104166666699, 0.18410416666666699, 0.187671875, 0.19107291666666698, 0.20330208333333302, 0.19827083333333304, 0.19460416666666697, 0.191671875, 0.18748958333333302, 0.18288020833333302, 0.17986979166666697, 0.177734375, 0.17532291666666697, 0.173223958333333, 0.17119791666666698, 0.169119791666667, 0.167192708333333, 0.164770833333333, 0.161864583333333, 0.158760416666667, 0.154598958333333, 0.15071875, 0.1461875, 0.142151041666667, 0.137692708333333, 0.131557291666667, 0.127890625, 0.12521875, 0.120479166666667, 0.11591145833333301, 0.111744791666667, 0.10759375, 0.104630208333333, 0.10371875, 0.106109375, 0.107333333333333, 0.10516666666666699, 0.10256770833333301, 0.10001041666666699, 0.097609375, 0.098265625, 0.0966614583333333, 0.0941979166666667, 0.0916197916666667, 0.0887083333333333, 0.0853645833333333, 0.0819739583333333, 0.078515625, 0.0750520833333333, 0.0715052083333333, 0.0676197916666667, 0.06746875, 0.0731510416666667, 0.0714270833333333, 0.06959375, 0.06740625, 0.0658385416666667, 0.0634739583333333, 0.0611145833333333, 0.0586458333333333, 0.0538697916666667, 0.0655729166666667, 0.0687447916666667, 0.076140625, 0.0766927083333333, 0.0861927083333333, 0.0890260416666667, 0.0988541666666667, 0.108489583333333, 0.123666666666667, 0.122692708333333, 0.124713541666667, 0.122296875, 0.11815625, 0.120703125, 0.14090625, 0.131520833333333, 0.125140625, 0.129317708333333, 0.130182291666667, 0.130807291666667, 0.12447395833333301, 0.129604166666667, 0.124484375, 0.119921875, 0.115734375, 0.112375, 0.112546875, 0.112234375, 0.11025, 0.146958333333333, 0.14106770833333301, 0.141947916666667, 0.136880208333333, 0.134270833333333, 0.157078125, 0.155432291666667, 0.151953125, 0.157098958333333, 0.157609375, 0.1554375, 0.169520833333333, 0.158630208333333, 0.151385416666667, 0.145859375, 0.140453125, 0.135442708333333, 0.130692708333333, 0.126703125, 0.097171875, 0.15088020833333302, 0.148864583333333, 0.157317708333333, 0.17308854166666698, 0.162109375, 0.154708333333333, 0.0313697916666667, 0.113588541666667, 0.13925, 0.144208333333333, 0.148057291666667, 0.146359375, 0.14378125, 0.13396875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0361666666666667, 0.12596875, 0.17390104166666698, 0.167442708333333, 0.162338541666667, 0.160755208333333, 0.160651041666667, 0.161255208333333, 0.161286458333333, 0.160536458333333, 0.164546875, 0.18268229166666697, 0.17516666666666697, 0.173359375, 0.18500520833333303, 0.17902604166666697, 0.17770833333333302, 0.206546875, 0.189802083333333, 0.18195833333333303, 0.177421875, 0.18901041666666699, 0.183921875, 0.18679166666666697, 0.183171875, 0.18019791666666699, 0.17653645833333304, 0.17295833333333302, 0.16980729166666697, 0.166973958333333, 0.16440625, 0.161958333333333, 0.159927083333333, 0.0066197916666666705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0738958333333333, 0.1609375, 0.16796354166666697, 0.170234375, 0.169921875, 0.168703125, 0.167244791666667, 0.166432291666667, 0.17419791666666698, 0.188307291666667, 0.188765625, 0.1830625, 0.17930208333333303, 0.17635416666666698, 0.17394791666666698, 0.17194791666666698, 0.170072916666667, 0.169578125, 0.169109375, 0.16815104166666697, 0.167010416666667, 0.165791666666667, 0.1644375, 0.16321875, 0.162104166666667, 0.161020833333333, 0.16044270833333302, 0.160578125, 0.165380208333333, 0.170640625, 0.153239583333333, 0.170046875, 0.16904166666666698, 0.167796875, 0.166546875, 0.17196875, 0.18318229166666697, 0.17884375, 0.176171875, 0.17396875, 0.17204166666666698, 0.170296875, 0.168703125, 0.18452083333333302, 0.20370833333333302, 0.19384375, 0.18882291666666698, 0.18508854166666697, 0.182244791666667, 0.17988541666666696, 0.17786979166666697, 0.17577083333333302, 0.173479166666667, 0.17133854166666698, 0.16939583333333302, 0.16834375, 0.167416666666667, 0.165708333333333, 0.162817708333333, 0.158697916666667, 0.155697916666667, 0.153989583333333, 0.153401041666667, 0.152401041666667, 0.151973958333333, 0.151541666666667, 0.15, 0.147713541666667, 0.144958333333333, 0.142645833333333, 0.140947916666667, 0.139317708333333, 0.138546875, 0.1383125]\n"
     ]
    }
   ],
   "source": [
    "# now check accuracy for test set...\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    y_pred = model(inputs.float())\n",
    "    \n",
    "    test_loss = criterion(y_pred, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred_cpu = y_pred.cpu()\n",
    "    pred_cpu = pred_cpu.detach().numpy().reshape((len(pred_cpu)))\n",
    "    pred_cpu = pred_cpu.tolist()\n",
    "    \n",
    "    \n",
    "    lab_cpu = labels.cpu()\n",
    "    lab_cpu = lab_cpu.numpy()\n",
    "    lab_cpu = lab_cpu.tolist()\n",
    "    \n",
    "    predictions.extend(pred_cpu)\n",
    "    actuals.extend(lab_cpu)\n",
    "    \n",
    "\n",
    "print(\"Test loss: \", test_losses)\n",
    "print(\"Mean test loss: \", np.mean(test_losses))\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n",
    "print()\n",
    "print(\"Actual values:\")\n",
    "print(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500ad7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
